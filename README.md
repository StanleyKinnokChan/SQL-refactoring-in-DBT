# SQL refactoring in DBT
This project aims at demostrating how to refactoring the Snowflake hard in DBT. By migrating the code into DBT, we can achieve:
- **Modularity**: Bulk code breaking down into smaller & reusable pieces
- **Documentation**: Webpage documentation was generated by command `dbt docs generate` and `dbt docs serve`
- **Testing and Validation**: Here is done by Audit-helper package to ensure both original SQL and refactored SQL queries the exactly same result set
- **Dependency Management**: The reference of source is located at source folder. It would be the only things needed to be modified if the source schema is changed. All models were referenced with Jinja. 
- **Version Control**: Using GIT

This repository mainly recorded how the original SQL code was modified step-by-step. To find out the changes for each commit, see the commit history of the folder **./demo_dbt_refactoring/models**.

## What is refactoring?
Refactoring refers to the process of restructuring and optimizing SQL code used in dbt models, macros, and other dbt artifacts to improve the performance, readability, maintainability, and overall quality of your data transformation workflows. dbt provides a framework and set of best practices for organizing, structuring, and managing SQL code used in data analytics pipelines. 

## Environment Details
The project was developed using **DBT-core version** 1.6.3 within an **Anaconda** virtual environment 2023.07. It connects to the Snowflake datawarehouse with PyPI package **dbt-snowflake**. See https://docs.getdbt.com/docs/core/connect-data-platform/snowflake-setup

## Step 0. Project setup and migrate legacy code to new model

#### Create snowflake DB and schema for this project

First create warehouse, database and schema decidated to this project in Snowflake. 
```sh
create warehouse demo_dbt WAREHOUSE_SIZE = Xsmall AUTO_SUSPEND = 60;
create database demo_dbt_refactoring;
create schema demo_dbt_refactoring.origin;
USE WAREHOUSE DEMO_DBT;
USE DEMO_DBT_REFACTORING.ORIGIN;
```
We have the following builk SQL, which has nested queries, messy cleaning & transformation logic and repeated referencing. It is used to calculate the customer lifetime value (clv_bad) based on the total amount paid in previous orders for the same customer, which requires 3 tables.

```sh
WITH paid_orders AS (
  SELECT 
    orders.id AS order_id, 
    orders.user_id AS customer_id, 
    orders.order_date AS order_placed_at, 
    orders.status AS order_status, 
    p.total_amount_paid, 
    p.payment_finalized_date, 
    C.first_name AS customer_first_name, 
    C.last_name AS customer_last_name 
  FROM 
    orders AS Orders 
    LEFT JOIN (
      SELECT 
        orderid AS order_id, 
        Max(created) AS payment_finalized_date, 
        Sum(amount) / 100.0 AS total_amount_paid 
      FROM 
        payments
      WHERE 
        status <> 'fail' 
      GROUP BY 
        1
    ) p ON orders.id = p.order_id 
    LEFT JOIN customers C ON orders.user_id = C.id
), 
customer_orders AS (
  SELECT 
    C.id AS customer_id, 
    Min(order_date) AS first_order_date, 
    Max(order_date) AS most_recent_order_date, 
    Count(orders.id) AS number_of_orders 
  FROM 
    customers C 
    LEFT JOIN orders AS Orders ON orders.user_id = C.id 
  GROUP BY 
    1
) 
SELECT 
  p.*, 
  Row_number() OVER (
    ORDER BY 
      p.order_id
  ) AS transaction_seq, 
  Row_number() OVER (
    partition BY customer_id 
    ORDER BY 
      p.order_id
  ) AS customer_sales_seq, 
  CASE WHEN c.first_order_date = p.order_placed_at THEN 'new' ELSE 'return' END AS nvsr, 
  x.clv_bad AS customer_lifetime_value, 
  c.first_order_date AS fdos 
FROM 
  paid_orders p 
  LEFT JOIN customer_orders AS c using (customer_id) 
  LEFT OUTER JOIN (
    SELECT 
      p.order_id, 
      Sum(t2.total_amount_paid) AS clv_bad 
    FROM 
      paid_orders p 
      LEFT JOIN paid_orders t2 ON p.customer_id = t2.customer_id 
      AND p.order_id >= t2.order_id 
    GROUP BY 
      1 
    ORDER BY 
      p.order_id
  ) x ON x.order_id = p.order_id 
ORDER BY 
  order_id
```

#### Set up the DBT project
The DBT project was initiated using `dbt innit`. Then three csv file **customers.csv** , **orders.csv** and **payments.csv** were put into the **seed** folder. These 3 csv file were imported into Snowflake table using `dbt seed` command. Next, **customer_orders.sql** was created within **/model/legacy** folder, which contains original code, while **fct_customer_orders.sql** was created within **/model/mart** folder, which contains the refactored code being modified step-by-step.
![My Remote Image](https://i.imgur.com/E0Hc3T7.png)

#### Audit
After each step of refactoring, `dbt run -m <model>` was run to ensure the model were compiled correctly and 

**Audit-help** package was also used for auditing purpose. This allows us to compare the result of original SQL with the the result from the refactored SQL to see if there is any queries difference. The audit was carried our using the following code, which created a materialize view in Snowflake. This will be performed for each of the subsequent steps
```SH
{% set old_etl_relation=ref('customer_orders') %} 

{% set dbt_relation=ref('fct_customer_orders') %}  {{ 

audit_helper.compare_relations(
        a_relation=old_etl_relation,
        b_relation=dbt_relation,
        primary_key="order_id"
    ) }}
```
![My Remote Image](https://i.imgur.com/VsQh38P.png)

https://i.imgur.com/Kn5W4Cm.png

## Step 1. Adding source reference
The table reference in the original code was replaced by Jinja reference {{ source:('<source>','table') }}.

![My Remote Image](https://i.imgur.com/Kn5W4Cm.png)

## Step 2. Import source CTEs at the top, subsequent code will reference these CTEs instead
As the CTE reference scattered over the code, the sources code were firstly queries as CTEs so that the latter source can all reference the same table at the top. 
![My Remote Image](https://i.imgur.com/lJrJ470.png)

## Step 3. Centralizing logic & Create staging models for reference
In the orginal code, there are a big chunk of code used to transform the source table into a more readable and usable format (e.g. rename column, reordering, data type casting). They are not a main part of busniess logic. As a result, we created 3 seperated model in **/models/staging** folder with source.yml file, which transformed the data in the source level so that this simplied our refactored code. As the source table queries moved to the staging model, our refactored code now refers the new staging models using the `{{ ref('<source>','<table>') }}`, instead of `{{ source('<model>') }}`. 

![My Remote Image](https://i.imgur.com/RPcpUbO.png)

The subqueries were also moved to the logcal transformation part of CTEs after defining source CTEs, which helps the readability of the code. 

## Step 4. Move reusable component to intermediate model & simplify the final model
To further simply the code, the logical tranformation was moved to the intermediate model `int_orders.sql` was created inside the folders `model/marts/intermediate/`. Also column reference from the original column names to the new column names ordering of window function subclause was written in a more explicit wayt.


## Documentation
Webpage documentation was generated by command `dbt docs generate` and `dbt docs serve`
![My Remote Image](https://i.imgur.com/a2OGdKE.png)
With the lineage graph
![My Remote Image](https://i.imgur.com/VtFjEyc.png)
